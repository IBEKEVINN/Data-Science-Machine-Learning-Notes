{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Model where the right answers are fed to the model so that it can learn how to adjust predictions. \n",
    "\n",
    "Two problems:\n",
    "- Regression - Produces a function for a continuous output.\n",
    "- Classification - Predict which discrete category a data point(s) belongs to. \n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Derive the structure of data points based on the relationship (e.g. clustering) of data. No feedback based on the prediction results. e.g. Common markets in pool of genes, isolating cocktail party voices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent algorithm is given by:\n",
    "\n",
    "$$ \\theta_{j} = \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0},\\theta_{1}) $$\n",
    "*$\\alpha$ - Learning rate (size of small step)*\n",
    "\n",
    "The iteration of gradient descent should be applied simultaenoulsy to each variable i.e. updating all values of $\\theta_{j}$ first before starting next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function: \n",
    "$$ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} * \\sum \\limits _{i=1} ^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^{2} $$\n",
    "\n",
    "### Linear Regression:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tactics below are used to vary the range of input variables so that they are all roughly the same (ideally -1 <= x <= 1)\n",
    "\n",
    "**Feature Scaling**: _Feature / Range_\n",
    "\n",
    "**Mean Normilisation**: _(Feature - Mean) / Range_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate ($\\alpha$)**: For a sufficiently small enough $\\alpha$, J{$\\theta$} should decrease.\n",
    "\n",
    "_Debugging gradient descent: Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA For Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis and BIRCH algorithms are better at identifying elongated groups\n",
    "\n",
    "K-Means algorithmsare better at identifying spherical shapes as it is based on equal variance and distance between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
