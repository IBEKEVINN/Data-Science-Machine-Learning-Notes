{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Model where the right answers are fed to the model so that it can learn how to adjust predictions. \n",
    "\n",
    "Two problems:\n",
    "- Regression - Produces a function for a continuous output.\n",
    "- Classification - Predict which discrete category a data point(s) belongs to. \n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Derive the structure of data points based on the relationship (e.g. clustering) of data. No feedback based on the prediction results. e.g. Common markets in pool of genes, isolating cocktail party voices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent algorithm is given by:\n",
    "\n",
    "$$ \\theta_{j} = \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0},\\theta_{1}) $$\n",
    "*$\\alpha$ - Learning rate (size of small step)*\n",
    "\n",
    "Where J is the value of the function to be minimised. The iteration of gradient descent should be applied simultaenoulsy to each variable i.e. updating all values of $\\theta_{j}$ first before starting next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Feature Transformation<br>**\n",
    "Numerical transformations and methods used to encode non-numerical features. Applied before implementing a model, including scaling, hashing and hot encoding.\n",
    "\n",
    "**2. Dimensionality Reduction<br>**\n",
    "Reduces the number of variables/features within a dataset. This process itself is usually a machine learning algorithm including PCA or LDA (Linear Discriminant Analysis). Transforms existing feature space into a new subset of features that no longer relate to the real world, they are mathematical objects that are related to original features. Lack of interpretability and often called __feature extraction__\n",
    "\n",
    "**3. Feature Selection<br>**\n",
    "Choosing among features. <br>\n",
    "        \n",
    "i) Filter Methods: 'Filters' out useful features by methods such as Chi^2, Anova. <br>\n",
    "\n",
    "ii) Wrapper Methods: Brute force training with a select subset of features, combined with evaluation to select features. Timeframe usually set by a specific performance metric. Includes forward feature selection and backward feature selection <br>\n",
    "\n",
    "iii) Implemented during model implementation to tweak model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
